{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53186427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12680\\803022442.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from TMfunctions import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "# Cross Validation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b226f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311ea545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b477737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605e1dc",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c07162",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/No PreProcessing/train.csv', index_col = 0)\n",
    "test = pd.read_csv('Data/No PreProcessing/test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f7f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf40694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into sentences\n",
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tot = []\n",
    "for el in df['medical_abstract']:\n",
    "    lst = tokenizer.tokenize(el)\n",
    "    tot += lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb31aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open(\"Data/sentences\", \"wb\") as fp:\n",
    "    pickle.dump(tot, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b47762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('Data/sentences', 'rb') as f:\n",
    "    tot = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb846ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81132"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612dbdd",
   "metadata": {},
   "source": [
    "# No Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a94c794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preProcessing\n",
    "def preprocess(text):\n",
    "    # Remove punctuation and other non-alphanumeric characters\n",
    "    text =  re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    return words\n",
    "    # Join the words back into a string\n",
    "    #return ' '.join(words)\n",
    "\n",
    "prep_tot = [preprocess(x) for x in tot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f313de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(prep_tot, min_count=5, vector_size=100, window=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "042c783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/No PreProcessing/train.csv', index_col = 0)\n",
    "test = pd.read_csv('Data/No PreProcessing/test.csv', index_col = 0)\n",
    "data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af6486b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50b0dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:29<00:00,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n",
      "{'DT': 0.454, 'RF': 0.504, 'NB': 0.555}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n",
      "{'DT': 0.471, 'RF': 0.518, 'NB': 0.566}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:34<00:00,  6.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n",
      "{'DT': 0.475, 'RF': 0.508, 'NB': 0.543}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:32<00:00,  6.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4:\n",
      "{'DT': 0.454, 'RF': 0.511, 'NB': 0.542}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:32<00:00,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5:\n",
      "{'DT': 0.461, 'RF': 0.519, 'NB': 0.555}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DT': 0.46299999999999997, 'RF': 0.512, 'NB': 0.5522000000000001}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "Kresults = []\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    fold += 1\n",
    "    train = data.loc[train_index]\n",
    "    test = data.loc[test_index]\n",
    "    \n",
    "    FE_train = pd.DataFrame(np.array([vectorize(doc) for doc in train['medical_abstract']]))\n",
    "    FE_test = pd.DataFrame(np.array([vectorize(doc) for doc in test['medical_abstract']]))\n",
    "    \n",
    "    Kresult = micro_f1(build_results(FE_train, FE_test, train, test))\n",
    "    Kresults.append(Kresult)\n",
    "    print(\"fold \" + str(fold) + \":\")\n",
    "    print(Kresult)\n",
    "    print(\" \")\n",
    "    \n",
    "    DT, RF, NB = 0, 0, 0\n",
    "    for result in Kresults:\n",
    "        DT += result['DT']\n",
    "        RF += result['RF']\n",
    "        NB += result['NB']\n",
    "    result = {'DT': DT/5, 'RF': RF/5, 'NB': NB/5}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e35c9d",
   "metadata": {},
   "source": [
    "# StopWords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "204becf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preProcessing\n",
    "def preprocess(text):\n",
    "    # Remove punctuation and other non-alphanumeric characters\n",
    "    text =  re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "no_stop = [preprocess(x) for x in tot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c6360fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open(\"Data/no_stop\", \"wb\") as fp:\n",
    "    pickle.dump(no_stop, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e201dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open(\"Data/no_stop\", \"rb\") as fp:\n",
    "    no_stop = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "299dfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(no_stop, min_count=5, vector_size=100, window=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0401f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/StopWords/train.csv', index_col = 0)\n",
    "test = pd.read_csv('Data/StopWords/test.csv', index_col = 0)\n",
    "data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20b0a1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:33<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n",
      "{'DT': 0.499, 'RF': 0.594, 'NB': 0.569}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:33<00:00,  6.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n",
      "{'DT': 0.54, 'RF': 0.598, 'NB': 0.577}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:35<00:00,  7.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n",
      "{'DT': 0.53, 'RF': 0.599, 'NB': 0.585}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:38<00:00,  7.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4:\n",
      "{'DT': 0.512, 'RF': 0.597, 'NB': 0.578}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:35<00:00,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5:\n",
      "{'DT': 0.507, 'RF': 0.59, 'NB': 0.567}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DT': 0.5176000000000001, 'RF': 0.5955999999999999, 'NB': 0.5751999999999999}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "Kresults = []\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    fold += 1\n",
    "    train = data.loc[train_index]\n",
    "    test = data.loc[test_index]\n",
    "    \n",
    "    FE_train = pd.DataFrame(np.array([vectorize(doc) for doc in train['medical_abstract']]))\n",
    "    FE_test = pd.DataFrame(np.array([vectorize(doc) for doc in test['medical_abstract']]))\n",
    "    \n",
    "    Kresult = micro_f1(build_results(FE_train, FE_test, train, test))\n",
    "    Kresults.append(Kresult)\n",
    "    print(\"fold \" + str(fold) + \":\")\n",
    "    print(Kresult)\n",
    "    print(\" \")\n",
    "    \n",
    "    DT, RF, NB = 0, 0, 0\n",
    "    for result in Kresults:\n",
    "        DT += result['DT']\n",
    "        RF += result['RF']\n",
    "        NB += result['NB']\n",
    "    result = {'DT': DT/5, 'RF': RF/5, 'NB': NB/5}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8725f2",
   "metadata": {},
   "source": [
    "# StopWords Removal + Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae24cf65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Apply Lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordlemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "\n",
    "lemm = [lemmatizer(x) for x in no_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33f7c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "#with open(\"Data/lemm\", \"wb\") as fp:\n",
    "#    pickle.dump(lemm, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f716326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/lemm\", \"rb\") as fp:\n",
    "    lemm = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3c19735",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(lemm, min_count=5, vector_size=100, window=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d49063",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/StopWords + Lemm/train.csv', index_col = 1)\n",
    "test = pd.read_csv('Data/StopWords + Lemm/test.csv', index_col = 1)\n",
    "data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4607cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:33<00:00,  6.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n",
      "{'DT': 0.533, 'RF': 0.596, 'NB': 0.569}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 2:\n",
      "{'DT': 0.519, 'RF': 0.595, 'NB': 0.58}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:32<00:00,  6.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 3:\n",
      "{'DT': 0.537, 'RF': 0.602, 'NB': 0.585}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:34<00:00,  6.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 4:\n",
      "{'DT': 0.511, 'RF': 0.607, 'NB': 0.583}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:31<00:00,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 5:\n",
      "{'DT': 0.524, 'RF': 0.594, 'NB': 0.57}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DT': 0.5248, 'RF': 0.5987999999999999, 'NB': 0.5774}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "Kresults = []\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    fold += 1\n",
    "    train = data.loc[train_index]\n",
    "    test = data.loc[test_index]\n",
    "    \n",
    "    FE_train = pd.DataFrame(np.array([vectorize(doc) for doc in train['medical_abstract']]))\n",
    "    FE_test = pd.DataFrame(np.array([vectorize(doc) for doc in test['medical_abstract']]))\n",
    "    \n",
    "    Kresult = micro_f1(build_results(FE_train, FE_test, train, test))\n",
    "    Kresults.append(Kresult)\n",
    "    print(\"fold \" + str(fold) + \":\")\n",
    "    print(Kresult)\n",
    "    print(\" \")\n",
    "    \n",
    "    DT, RF, NB = 0, 0, 0\n",
    "    for result in Kresults:\n",
    "        DT += result['DT']\n",
    "        RF += result['RF']\n",
    "        NB += result['NB']\n",
    "    result = {'DT': DT/5, 'RF': RF/5, 'NB': NB/5}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fdcf7",
   "metadata": {},
   "source": [
    "### Evaluating the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8508939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_size: 50, window: 5\n",
      "{'DT': 0.5174000000000001, 'RF': 0.5859999999999999, 'NB': 0.5775999999999999}\n",
      " \n",
      "vector_size: 50, window: 7\n",
      "{'DT': 0.5204000000000001, 'RF': 0.596, 'NB': 0.5835999999999999}\n",
      " \n",
      "vector_size: 50, window: 10\n",
      "{'DT': 0.5240000000000001, 'RF': 0.5982, 'NB': 0.5921999999999998}\n",
      " \n",
      "vector_size: 100, window: 5\n",
      "{'DT': 0.52, 'RF': 0.591, 'NB': 0.5755999999999999}\n",
      " \n",
      "vector_size: 100, window: 7\n",
      "{'DT': 0.522, 'RF': 0.5984, 'NB': 0.5771999999999999}\n",
      " \n",
      "vector_size: 100, window: 10\n",
      "{'DT': 0.5306, 'RF': 0.6058, 'NB': 0.5833999999999999}\n",
      " \n",
      "vector_size: 200, window: 5\n",
      "{'DT': 0.5264, 'RF': 0.5964, 'NB': 0.5668}\n",
      " \n",
      "vector_size: 200, window: 7\n",
      "{'DT': 0.5284000000000001, 'RF': 0.6035999999999999, 'NB': 0.571}\n",
      " \n",
      "vector_size: 200, window: 10\n",
      "{'DT': 0.536, 'RF': 0.6138, 'NB': 0.5756}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for vector_size in [50, 100, 200]:\n",
    "    for window in [5, 7, 10]:\n",
    "        model = Word2Vec(lemm, min_count=5, vector_size=vector_size, window=window)\n",
    "        \n",
    "        kf = KFold(n_splits=5)\n",
    "        fold = 0\n",
    "        Kresults = []\n",
    "\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            fold += 1\n",
    "            train = data.loc[train_index]\n",
    "            test = data.loc[test_index]\n",
    "\n",
    "            FE_train = pd.DataFrame(np.array([vectorize(doc) for doc in train['medical_abstract']]))\n",
    "            FE_test = pd.DataFrame(np.array([vectorize(doc) for doc in test['medical_abstract']]))\n",
    "\n",
    "            Kresult = micro_f1(build_results(FE_train, FE_test, train, test))\n",
    "            Kresults.append(Kresult)\n",
    "\n",
    "            DT, RF, NB = 0, 0, 0\n",
    "            for result in Kresults:\n",
    "                DT += result['DT']\n",
    "                RF += result['RF']\n",
    "                NB += result['NB']\n",
    "            result = {'DT': DT/5, 'RF': RF/5, 'NB': NB/5}\n",
    "        print('vector_size: ' + str(vector_size) + ', window: ' + str(window))\n",
    "        print(result)\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1feaf",
   "metadata": {},
   "source": [
    "### Trying with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b949361",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(lemm, min_count=5, vector_size=200, window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca919b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/StopWords + Lemm/train.csv', index_col = 1)\n",
    "test = pd.read_csv('Data/StopWords + Lemm/test.csv', index_col = 1)\n",
    "data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9ed34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1:\n",
      "{'DT': 0.534, 'RF': 0.61, 'NB': 0.565, 'SVM': 0.627}\n",
      " \n",
      "fold 2:\n",
      "{'DT': 0.539, 'RF': 0.613, 'NB': 0.581, 'SVM': 0.63}\n",
      " \n",
      "fold 3:\n",
      "{'DT': 0.544, 'RF': 0.619, 'NB': 0.585, 'SVM': 0.641}\n",
      " \n",
      "fold 4:\n",
      "{'DT': 0.53, 'RF': 0.614, 'NB': 0.581, 'SVM': 0.639}\n",
      " \n",
      "fold 5:\n",
      "{'DT': 0.524, 'RF': 0.596, 'NB': 0.567, 'SVM': 0.628}\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DT': 0.5342, 'RF': 0.6104, 'NB': 0.5757999999999999, 'SVM': 0.633}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "fold = 0\n",
    "Kresults = []\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    fold += 1\n",
    "    train = data.loc[train_index]\n",
    "    test = data.loc[test_index]\n",
    "    \n",
    "    FE_train = pd.DataFrame(np.array([vectorize(doc) for doc in train['medical_abstract']]))\n",
    "    FE_test = pd.DataFrame(np.array([vectorize(doc) for doc in test['medical_abstract']]))\n",
    "    \n",
    "    Kresult = micro_f1SVM(build_resultsSVM(FE_train, FE_test, train, test))\n",
    "    Kresults.append(Kresult)\n",
    "    print(\"fold \" + str(fold) + \":\")\n",
    "    print(Kresult)\n",
    "    print(\" \")\n",
    "    \n",
    "    DT, RF, NB, SVM = 0, 0, 0, 0\n",
    "    for result in Kresults:\n",
    "        DT += result['DT']\n",
    "        RF += result['RF']\n",
    "        NB += result['NB']\n",
    "        SVM += result['SVM']\n",
    "    result = {'DT': DT/5, 'RF': RF/5, 'NB': NB/5, 'SVM': SVM/5}\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMiningK",
   "language": "python",
   "name": "textminingk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
